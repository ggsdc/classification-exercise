\documentclass[a4paper,11pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[dvipdfmx]{graphicx}
\usepackage{graphics,latexsym}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage[dvips]{color}
\usepackage{subfigure}
\usepackage{verbatim}


\usepackage{graphicx}          % Gráficos 
\usepackage{geometry}
\usepackage{fancyhdr}          % Personalizar estilo página
%\usepackage{fancyheadings}     % Para los headers y footers - Deprecated
\usepackage{color}             % Foreground and background color management
\usepackage[usenames,dvipsnames,table]{xcolor}   % Permite modificar tonos de color de forma sencilla
\usepackage{import}            % Importar ficheros de carpetas dentro de este proyecto
\usepackage{listings}          % Permite pegar texto sin formato. Específico para pegar código
\usepackage{placeins}          % Float barriers para imágenes/tablas/etc
\usepackage{float}             % Usar [H] para tablas
\usepackage{amsmath}           % Fórmulas matemáticas
\usepackage{verbatimbox}       % Encuandrar en cajas texto sin formato (verbatim)
\usepackage{tikz}              % Para graficar desde latex
\usepackage{tikz-qtree}        % Para dibujar esquemas en forma de árbol
\usepackage{soul}              % Para separar con guiones las palabras cuando no caben en la línea de texto
\usepackage{titlesec}          % Select alternative section titles
\usepackage{hyperref}          % Vínculos
\usepackage{nameref}           % Permite poner referencias con nombre en vez de con número.



\bibpunct{(}{)}{;}{a}{,}{,}

\textheight 24cm \textwidth 17cm \topmargin-2cm
%% \evensidemargin   -0.25cm
\oddsidemargin-0.2cm
%\pagestyle{empty}
\renewcommand{\baselinestretch}{1}

\begin{document}

\title{Classification and clustering of clients who will churn with various algorithms}

\author{{Guillermo González-Santander de la Cruz}\\
{\small Computational Intelligence Group, Departamento de Inteligencia Artificial, Universidad Politécnica de Madrid, Spain}}

\date{2020-01-08}
\maketitle

%\title{}

%\address{}

\begin{abstract}
\end{abstract}


\ \\
KEY WORDS: Churn prediction; Classification; Clustering; Cross-valdiation; Grid-search

\section{Introduction}

The aim of this work is to predict which clients of a telecoms company are going to churn and which are not. In particular the dataset comes IBM Sample Data Sets.
We will use both classification and clustering to try asses two different things. First, with classification, we will try to build a model that tells us if a customer is going to left the services company based on the training data and available information; second, with clustering, we will try to divide the customers in two groups, which clearly separates the customers that left the company from those that still use the services of the company.
For both task we will use different methods in order to find which gives es the best accuracy along with explain-ability and speed.
All the algorithms that need hiper-parameter tuning have been tried with ten fold cross-validation.

\section{Preliminaries}

\section{Dataset and analysis}

The dataset used for this work comes from the IBM Sample Data Sets (\url{https://community.ibm.com/community/user/businessanalytics/blogs/steven-macko/2017/06/19/guide-to-ibm-cognos-analytics-sample-data-sets}), in the website a more recent version of the dataset can be found.

The dataset does not have any missing value and has both numerical an qualitative attributes. The details of the attributes are:

\begin{enumerate}
	\item \textbf{customerID}: Unique ID code for each customer. It will not be used in the analysis.
	\item \textbf{gender}: Qualitative variable if the customer is either female or male. It is fairly balanced (49.52\% female).
	\item \textbf{SeniorCitizen}: Binary variable if the customer is considered a senior citizen. It is not well balanced (16.21\% senior).
	\item \textbf{Partner}: Qualitative variable (yes / no) if the customer has a partner. It is well balanced (45.94\% yes).
	\item \textbf{Dependents}: Qualitative variable (yes / no) if the customer has dependents. It is not well balanced (29,95\% yes).
	\item \textbf{tenure}: Quantitative variable (integer) with the amount of months the customer has been with the company.
	\item \textbf{PhoneService}: Qualitative variable (yes / no) if the customer has phone service with the company.
	\item \textbf{MultipleLines}: Qualitative variable (yes / no / no phone service) if the customer has multiple phone lines.
	\item \textbf{InternetService}: Qualitative variable (DSL / Fiber optic/no) if the customer has internet service with the company.
	\item \textbf{OnlineSecurity}: Qualitative variable (yes / no / no internet service) if the customer has online security with the company.
	\item \textbf{OnlineBackup}: Qualitative variable (yes / no / no internet service) if the customer has online backup with the company.
	\item \textbf{DeviceProtection}: Qualitative variable (yes / no / no internet service) if the customer has device protection with the company.
	\item \textbf{TechSupport}: Qualitative variable (yes / no / no internet service) if the customer has tech support with the company.
	\item \textbf{StreamingTV}: Qualitative variable (yes / no / no internet service) if the customer has streaming tv with the company.
	\item \textbf{StreamingMovies}: Qualitative variable (yes / no / no internet service) if the customer has streaming movies with the company.
	\item \textbf{Contract}: Qualitative variable (Month-to-month / One year / Two year) indicating the type of contratc that the customer has.
	\item \textbf{PaperlessBilling}: Qualitative variable (yes / no) if the customer has paperless billing or not.
	\item \textbf{PaymentMethod}: Qualitative variable (Bank transfer / credit card / electronic check / mailed check) indicating the payment method used by the customer.
	\item \textbf{MonthlyCharges}: Quantitative variable (real) with the amount the customer has to pay everymonth.
	\item \textbf{TotalCharges}: Quantitative variable (real) with the amount the customer has paid during their tenure with the company.
	\item \textbf{Churn}: Qualitative variable (yes / no) if the customer has left the company.
\end{enumerate}

In order to apply the algorithms for classification and clustering two pre-processes are going to be applied: the quantitative variables of $n$ levels are going to be one-hot encoded in $n-1$ variables; and all the variables are going to be scaled to the [0, 1] range.

\section{Methodology}

\subsection{Classification}

We are going to be apply the following algorithms for the classification part of the work: \textit{k-nearest-neighbors} (\textit{knn}, with a similar approach as condensed nearest neighbors algorithm, \cite{hart1968}), classification trees (with the CART algorithm, \cite{breiman1984}), a naive bayes classifier (\cite{minsky1961}), logistic regression, rule induction (RIPPER, \cite{cohen1995}), and a series of metaclassifiers: bagging of classification trees (\cite{breiman1996}), random forest (\cite{breiman2001}) and gradient boosting with classification trees (\cite{freund1997}).

For all the algorithms that are going to be used we are going to apply the same methodology:

\begin{enumerate}
	\item First we split the dataset into train and test with an 80/20 split.
	\item We train the model on the training data (80\%). For this we use a 10-fold cross-validation to check that the model does not overfit and to select the best parameters for those algorithms that require parameter tuning.
	\item We evaluate the results (accuracy and confusion matrix) over the 20\% of data that the models have not been trained on.
	\item We also evaluate the training time for each model and the explainability of the model.
\end{enumerate}

This methodology is repeated across different versions of the same dataset:

\begin{itemize}
	\item The original dataset with all 26 attributes.
	\item A filtered dataset with the 6, 13 and 19 (25\%, 50\% and 75\%) more meaningful attributes measured with an univariate ANOVA F-value test.
	\item A wrapped dataset with the 6, 13, and 19 more meaningful attributes gotten with a stepwise logistic regression model.
	\item A filtered dataset, processed to include the interactions between attributes, with the 6, 13 and 19 more meaningful attributes  measured with an univariate ANOVA F-value test.
	\item A wrapped dataset, processed to include the interactions between attributes, with the 6, 13, and 19 more meaningful attributes gotten with a stepwise logistic regression model.
\end{itemize}

\subsection{Clustering}

For the clustering we are going to build the clusters with the whole dataset and measure the accuracy of the clustering method with the Adjusted Rand Index (ARI) between the cluster label and the real label.

The clustering is going to be applied over the same datasets as the classification algorithms.

The clustering methods to be used are k-means and hierarchical agglomerative clustering (HAC), for these methods different configurations are going to be tested with cross-validation to select the best one. For k-means we are going to try two different initialization methods (random and kmeans++) and for HAC we are going to try with different distances (manhattan and euclidean) and with different linkages (average, complete and single).

\section{Classification algorithms results}

\subsection{Full dataset}

In table \ref{class:full} can be seen the results of all algorithms for the full dataset.

In all the tables the following notation is used: TP as true positive, FP as false positive, FN as false negative, TN as true negative, ACC as accuracy, PR as precision, RC as recall, T as the whole training and testing time, TpC time per combination of parameters tested (time per configuration). Both times are measured in seconds.

\begin{table}
\centering

\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
\hline

\textbf{Model} & \textbf{TP} & \textbf{FP} & \textbf{FN} & \textbf{TN} & \textbf{ACC} & \textbf{PR} & \textbf{RC} & \textbf{F1-Score} & \textbf{T} & \textbf{TpC} \\ \hline
KNN & 790 & 229 & 204 & 186 & 0.6927 & 0.7753 & 0.7948 & 0.7849 & 4981.69 & 2490.85 \\ \hline
Class. tree & 801 & 216 & 201 & 191 & 0.7040 & 0.7876 & 0.7994 & 0.7935 & 1.06 & \textbf{0.03} \\ \hline
Naive Bayes & 711 & 306 & 69 & 323 & 0.7339 & 0.6991 & \textbf{0.9115} & 0.7913 & \textbf{0.07} & 0.07 \\ \hline
Rule induction & 1 & 1 & 1 & 1 & 0.5000 & 0.5000 & 0.5000 & 0.5000 & 0.00 \\ \hline
Log. regression & 908 & 109 & 179 & 213 & \textbf{0.7956} & 0.8928 & 0.8353 & \textbf{0.8631} & 2.46 & 2.46 \\ \hline
Bagging & 925 & 92 & 203 & 189 & 0.7906 & \textbf{0.9095} & 0.8200 & 0.8625 & 705.67 & 26.14 \\ \hline
Random forest & 900 & 117 & 192 & 200 & 0.7807 & 0.8850 & 0.8242 & 0.8535 & 567.72 & 23.66 \\ \hline
AdaBoosting & 856 & 161 & 191 & 201 & 0.7502 & 0.8417 & 0.8176 & 0.8295 & 582.87 & 36.43 \\ \hline

\end{tabular}
\caption{Results for the classification methods for the full dataset}
\label{class:full}
\end{table}

As it can be seen logistic regression provides the best accuracy and f1-score, while bagging provides the best precision and Naive Bayes the best recall. 

While Naive Bayes is the fastest model, it should be taken into account that these are the training and testing times and for some algorithms a lot of parameter settings are being tried out in order to get the best results, for example, while logistic regression is only run once for each fold, classification trees are run forty times or bagging is run 29 times with configuration up to 300 individual estimators.

As it can be seen the metaclassifier methods can achieve better results than just one tree using configurations up to 500 individual trees but taking a lot of more time. The configurations of these models are tricky and can be searched with various methods, in this case, as it has been said we have used grid search over a small set of predefined values.

Even though their results are better than the classification tree, they are on par with the logistic regression, being the latter an easier model to explain to a third party or to understand from the researcher point of view.

\subsection{Univariate filter and wrapper}

For the univariate filter the selected variables have been: tenure, fiber, month-to-month, two-year, electronic check and total charges for the 6 most important attributes. This get expanded with monthly charges, partner, phone, security, support, one-year contract and paperless billing to get up to 13 attributes. And finally it gets expanded with senior citizen, dsl, online backup, bank transfer, credit card payment and mailed check to get up up to 19 attributes, all with p-values inferior to 1.06e-009.

In the following tables (\ref{class:f25u}, \ref{class:f50u} and \ref{class:f75u}) we can see the results of the univariate filter dataset and all the methods.

The results are slightly worse with 6 attributes than with the full dataset, while with 13 attributes gets better, but decreases again for 19 attributes and for the full dataset.

\begin{table}
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
\hline

\textbf{Model} & \textbf{TP} & \textbf{FP} & \textbf{FN} & \textbf{TN} & \textbf{ACC} & \textbf{PR} & \textbf{RC} & \textbf{F1-Score} & \textbf{T} & \textbf{TpC} \\ \hline
KNN & 808 & 211 & 214 & 176 & 0.6984 & 0.7929 & 0.7906 & 0.7918 & 4328.74 & 2164.37 \\ \hline
Class. tree & 813 & 204 & 219 & 173 & 0.6998 & 0.7994 & 0.7878 & 0.7936 & 0.45 & \textbf{0.01} \\ \hline
Naive Bayes & 614 & 403 & 55 & 337 & 0.6749 & 0.6037 & \textbf{0.9178} & 0.7284 & \textbf{0.04} & 0.04 \\ \hline
Rule induction & 1 & 1 & 1 & 1 & 0.5000 & 0.5000 & 0.5000 & 0.5000 & 0.00 \\ \hline
Log. regression & 923 & 94 & 203 & 189 & \textbf{0.7892} & 0.9076 & 0.8197 & \textbf{0.8614} & 0.21 & 0.21 \\ \hline
Bagging & 941 & 76 & 261 & 131 & 0.7608 & \textbf{0.9253} & 0.7829 & 0.8481 & 325.35 & 12.05 \\ \hline
Random forest & 860 & 157 & 232 & 160 & 0.7239 & 0.8456 & 0.7875 & 0.8156 & 358.31 & 14.93 \\ \hline
AdaBoosting & 825 & 192 & 221 & 171 & 0.7069 & 0.8112 & 0.7887 & 0.7998 & 412.82 & 25.80 \\ \hline

\end{tabular}
\caption{Results for the univariate filter with 6 attributes}
\label{class:f25u}
\end{table}

\begin{table}
\centering

\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
\hline

\textbf{Model} & \textbf{TP} & \textbf{FP} & \textbf{FN} & \textbf{TN} & \textbf{ACC} & \textbf{PR} & \textbf{RC} & \textbf{F1-Score} & \textbf{T} & \textbf{TpC} \\ \hline
KNN & 811 & 208 & 193 & 197 & 0.7154 & 0.7959 & 0.8078 & 0.8018 & 4257.21 & 2128.61 \\ \hline
Class. Tree & 820 & 197 & 203 & 189 & 0.7161 & 0.8063 & 0.8016 & 0.8039 & 0.79 & \textbf{0.02} \\ \hline
Naive Bayes & 671 & 346 & 60 & 332 & 0.7119 & 0.6598 & \textbf{0.9179} & 0.7677 & \textbf{0.04} & 0.04 \\ \hline
Rule induction & 1 & 1 & 1 & 1 & 0.5000 & 0.5000 & 0.5000 & 0.5000 & 0.00 \\ \hline
Log. regression & 922 & 95 & 195 & 197 & \textbf{0.7942} & 0.9066 & 0.8254 & \textbf{0.8641} & 0.47 & 0.47 \\ \hline
Bagging & 936 & 81 & 225 & 167 & 0.7828 & \textbf{0.9204} & 0.8062 & 0.8595 & 521.71 & 19.32 \\ \hline
Random forest & 900 & 117 & 201 & 191 & 0.7743 & 0.8850 & 0.8174 & 0.8499 & 468.54 & 19.52 \\ \hline
AdaBoosting & 874 & 143 & 207 & 185 & 0.7516 & 0.8594 & 0.8085 & 0.8332 & 618.66 & 38.67 \\ \hline

\end{tabular}
\caption{Results for the univariate filter with 13 attributes}
\label{class:f50u}
\end{table}

\begin{table}
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
\hline

\textbf{Model} & \textbf{TP} & \textbf{FP} & \textbf{FN} & \textbf{TN} & \textbf{ACC} & \textbf{PR} & \textbf{RC} & \textbf{F1-Score} & \textbf{T} & \textbf{TpC} \\ \hline
KNN & 801 & 218 & 198 & 192 & 0.7048 & 0.7861 & 0.8018 & 0.7939 & 4568.64 & 2284.32 \\ \hline
Class. Tree & 833 & 184 & 182 & 210 & 0.7402 & 0.8191 & 0.8207 & 0.8199 & 0.72 & \textbf{0.02} \\ \hline
Naive Bayes & 703 & 314 & 68 & 324 & 0.7289 & 0.6912 & \textbf{0.9118} & 0.7864 & \textbf{0.05} & 0.05 \\ \hline
Rule induction & 1 & 1 & 1 & 1 & 0.5000 & 0.5000 & 0.5000 & 0.5000 & 0.00 \\ \hline
Log. regression & 910 & 107 & 181 & 211 & \textbf{0.7956} & 0.8948 & 0.8341 & \textbf{0.8634} & 0.91 & 0.91 \\ \hline
Bagging & 927 & 90 & 215 & 177 & 0.7835 & \textbf{0.9115} & 0.8117 & 0.8587 & 636.76 & 23.58 \\ \hline
Random forest & 907 & 110 & 190 & 202 & 0.7871 & 0.8918 & 0.8268 & 0.8581 & 528.21 & 22.01 \\ \hline
AdaBoosting & 870 & 147 & 193 & 199 & 0.7587 & 0.8555 & 0.8184 & 0.8365 & 587.73 & 36.73 \\ \hline

\end{tabular}
\caption{Results for the univariate filter with 19 attributes}
\label{class:f75u}
\end{table}

For the univariate wrapper with stepwise logistic regression, the selected variables have been: senior citizen, tenure, monthly payments, phone, total charges and two years contract for the 6 most important attributes.This gets expanded with: male, female, partner, fiber, tv, bank transfer and mailed check for the 13 attributes. Finally this gets expanded with: multiple lines, movies, monthly contract, one-year contract, credit card payments and electronic check payment for the 19 attributes model.

The results of the models for these datasets can be seen in the following tables (\ref{class:w25u}, \ref{class:w50u} and \ref{class:w75u})

\begin{table}
\centering

\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
\hline

\textbf{Model} & \textbf{TP} & \textbf{FP} & \textbf{FN} & \textbf{TN} & \textbf{ACC} & \textbf{PR} & \textbf{RC} & \textbf{F1-Score} & \textbf{T} & \textbf{TpC} \\ \hline
KNN & 797 & 222 & 196 & 194 & 0.7033 & 0.7821 & 0.8026 & 0.7922 & 5396.00 & 2698.00 \\ \hline
Class. Tree & 808 & 209 & 189 & 203 & 0.7175 & 0.7945 & 0.8104 & 0.8024 & 0.69 & \textbf{0.02} \\ \hline
Naive Bayes & 642 & 375 & 78 & 314 & 0.6785 & 0.6313 & \textbf{0.8917} & 0.7392 & \textbf{0.04} & 0.04 \\ \hline
Rule induction & 1 & 1 & 1 & 1 & 0.5000 & 0.5000 & 0.5000 & 0.5000 & 0.00 \\ \hline
Log. regression & 933 & 84 & 216 & 176 & \textbf{0.7871} & 0.9174 & 0.8120 & \textbf{0.8615} & 0.23 & 0.23 \\ \hline
Bagging & 905 & 112 & 219 & 173 & 0.7651 & \textbf{0.8899} & 0.8052 & 0.8454 & 455.60 & 16.87 \\ \hline
Random forest & 879 & 138 & 215 & 177 & 0.7495 & 0.8643 & 0.8035 & 0.8328 & 437.30 & 18.22 \\ \hline
AdaBoosting & 851 & 166 & 203 & 189 & 0.7381 & 0.8368 & 0.8074 & 0.8218 & 498.04 & 31.13 \\ \hline

\end{tabular}
\caption{Results for the univariate wrapper with 6 attributes}
\label{class:w25u}
\end{table}

\begin{table}
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
\hline

\textbf{Model} & \textbf{TP} & \textbf{FP} & \textbf{FN} & \textbf{TN} & \textbf{ACC} & \textbf{PR} & \textbf{RC} & \textbf{F1-Score} & \textbf{T} & \textbf{TpC} \\ \hline
KNN & 806 & 213 & 198 & 192 & 0.7083 & 0.7910 & 0.8028 & 0.7968 & 4261.04 & 2130.52 \\ \hline
Class. Tree & 824 & 193 & 192 & 200 & 0.7268 & 0.8102 & 0.8110 & 0.8106 & 0.88 & \textbf{0.02} \\ \hline
Naive Bayes & 736 & 281 & 101 & 291 & 0.7289 & 0.7237 & \textbf{0.8793} & 0.7940 & \textbf{0.05} & 0.05 \\ \hline
Rule induction & 1 & 1 & 1 & 1 & 0.5000 & 0.5000 & 0.5000 & 0.5000 & 0.00 \\ \hline
Log. regression & 923 & 94 & 206 & 286 & \textbf{0.7871} & \textbf{0.9076} & 0.8175 & \textbf{0.8602} & 0.53 & 0.53 \\ \hline
Bagging & 920 & 97 & 215 & 177 & 0.7786 & 0.9046 & 0.8106 & 0.8550 & 545.66 & 20.21 \\ \hline
Random forest & 890 & 127 & 210 & 191 & 0.7623 & 0.8751 & 0.8091 & 0.8408 & 504.90 & 21.04 \\ \hline
AdaBoosting & 889 & 128 & 207 & 185 & 0.7622 & 0.8741 & 0.8111 & 0.8415 & 506.07 & 31.63 \\ \hline

\end{tabular}
\caption{Results for the univariate wrapper with 13 attributes}
\label{class:w50u}
\end{table}

\begin{table}
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
\hline

\textbf{Model} & \textbf{TP} & \textbf{FP} & \textbf{FN} & \textbf{TN} & \textbf{ACC} & \textbf{PR} & \textbf{RC} & \textbf{F1-Score} & \textbf{T} & \textbf{TpC} \\ \hline
KNN & 816 & 203 & 213 & 177 & 0.7048 & 0.8008 & 0.7930 & 0.7969 & 7861.42 & 3930.71 \\ \hline
Class. Tree & 823 & 194 & 205 & 187 & 0.7168 & 0.8092 & 0.8006 & 0.8049 & 1.00 & \textbf{0.03} \\ \hline
Naive Bayes & 687 & 330 & 68 & 324 & 0.7175 & 0.6755 & \textbf{0.9099} & 0.7754 & \textbf{0.06} & 0.06 \\ \hline
Rule induction & 1 & 1 & 1 & 1 & 0.5000 & 0.5000 & 0.5000 & 0.5000 & 0.00 \\ \hline
Log. regression & 911 & 106 & 187 & 205 & \textbf{0.7921} & 0.8958 & 0.8297 & \textbf{0.8615} & 0.80 & 0.80 \\ \hline
Bagging & 925 & 92 & 224 & 168 & 0.7757 & \textbf{0.9095} & 0.8050 & 0.8541 & 663.69 & 24.58 \\ \hline
Random forest & 877 & 140 & 195 & 197 & 0.7622 & 0.8623 & 0.8181 & 0.8396 & 541.55 & 22.56 \\ \hline
AdaBoosting & 833 & 184 & 215 & 177 & 0.7168 & 0.8191 & 0.7948 & 0.8068 & 644.50 & 40.28 \\ \hline

\end{tabular}
\caption{Results for the univariate wrapper with 19 attributes}
\label{class:w75u}
\end{table}

Again the results are in the same line as with the filtering, getting the best results with the 13 attributes dataset.

\subsection{Multivariate filter and wrapper}

For the multivariate filter and wrapper the interactions between attributes have been included, but all those that do not have enough variance have been left out in order to be easier to process the data.

The results for the filtering and wrapper are pretty similar to the ones already  getting the best results with the filter and the 13 attribute model with logistic regression (see table \ref{class:f50m}) and with the wrapping with 19 attributes and the logistic regression (see table \ref{w75m}).

\begin{table}
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
\hline

\textbf{Model} & \textbf{TP} & \textbf{FP} & \textbf{FN} & \textbf{TN} & \textbf{ACC} & \textbf{PR} & \textbf{RC} & \textbf{F1-Score} & \textbf{T} & \textbf{TpC} \\ \hline
KNN & 815 & 204 & 213 & 177 & 0.7040 & 0.7998 & 0.7928 & 0.7963 & 6505.00 & 3252.50 \\ \hline
Class. Tree & 852 & 165 & 202 & 190 & 0.7395 & 0.8378 & 0.8083 & 0.8228 & 0.84 & \textbf{0.02} \\ \hline
Naive Bayes & 770 & 247 & 112 & 280 & 0.7452 & 0.7571 & \textbf{0.8730} & 0.8110 & \textbf{0.10} & 0.10 \\ \hline
Rule induction & 1 & 1 & 1 & 1 & 0.5000 & 0.5000 & 0.5000 & 0.5000 & 0.00 \\ \hline
Log. regression & 915 & 102 & 186 & 206 & \textbf{0.7956} & \textbf{0.8997} & 0.8311 & \textbf{0.8640} & 0.48 & 0.48 \\ \hline
Bagging & 901 & 116 & 207 & 185 & 0.7708 & 0.8859 & 0.8132 & 0.8480 & 554.27 & 20.53 \\ \hline
Random forest & 864 & 153 & 201 & 191 & 0.7488 & 0.8496 & 0.8113 & 0.8300 & 503.75 & 20.99 \\ \hline
AdaBoosting & 856 & 161 & 205 & 187 & 0.7402 & 0.8417 & 0.8068 & 0.8239 & 534.00 & 33.38 \\ \hline

\end{tabular}
\caption{Results for the multivariate filter with 13 attributes}
\label{class:f50m}
\end{table}

\begin{table}
\centering
\caption{TableName}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
\hline

\textbf{Model} & \textbf{TP} & \textbf{FP} & \textbf{FN} & \textbf{TN} & \textbf{ACC} & \textbf{PR} & \textbf{RC} & \textbf{F1-Score} & \textbf{T} & \textbf{TpC} \\ \hline
KNN & 791 & 228 & 206 & 184 & 0.6920 & 0.7763 & 0.7934 & 0.7847 & 6544.94 & 3272.47 \\ \hline
Class. Tree & 854 & 163 & 203 & 189 & 0.7402 & 0.8397 & 0.8079 & 0.8235 & 0.78 & \textbf{0.02} \\ \hline
Naive Bayes & 662 & 355 & 92 & 300 & 0.6828 & 0.6509 & \textbf{0.8780} & 0.7476 & \textbf{0.10} & 0.10 \\ \hline
Rule induction & 1 & 1 & 1 & 1 & 0.5000 & 0.5000 & 0.5000 & 0.5000 & 0.00 \\ \hline
Log. regression & 907 & 110 & 185 & 207 & \textbf{0.7906} & 0.8918 & 0.8306 & \textbf{0.8601} & 0.44 & 0.44 \\ \hline
Bagging & 946 & 71 & 264 & 128 & 0.7622 & \textbf{0.9302} & 0.7818 & 0.8496 & 597.30 & 22.12 \\ \hline
Random forest & 894 & 123 & 217 & 175 & 0.7587 & 0.8791 & 0.8047 & 0.8402 & 532.52 & 22.19 \\ \hline
AdaBoosting & 882 & 135 & 222 & 170 & 0.7466 & 0.8673 & 0.7989 & 0.8317 & 498.40 & 31.15 \\ \hline

\end{tabular}
\caption{Results for the multivariate wrapper with 19 attributes}
\label{class:w75m}
\end{table}

\section{Clustering algorithms results}

\section{Conclusions and future work}

For classification algorithms it has been seen almost all model perform similarly for this dataset, having particularly good results logistic regression, Naive Bayes and bagging. Instead of using all the dataset, using a subset of features can help, both in time to train and test the model, as well in the results gotten by the models.

Using other strategy to find the best values for the parameters of the metaclassifiers could be another way to improve the results obtained. This could be used in future work, for example, either apply a bigger grid search with more values for the parameters, using Bayesian optimization or using a genetic algorithm.



\section{Acknowledgments}

Thanks to IBM for providing the dataset.
Thanks to all contributors of the libraries used in python to develop these models, particularly sklearn, mlxtend and pandas.
All code developed can be seen in the following repository: \url{https://github.com/ggsdc/classification-exercise}.


\bibliographystyle{plainnat}
\begin{thebibliography}{}

\bibitem[Breiman \textit{et al}, 1984]{breiman1984}
Breiman, L., Friedman, J. H., Olshen, R., \& Stone, C. J. (1984). \textit{Classification and Regression Trees}.

\bibitem[Breiman, 1996]{breiman1996}
Breiman, L. (1996). Bagging predictors. Machine learning, 24(2), 123-140

\bibitem[Breiman, 2001]{breiman2001}
Breiman, L. (2001). Random forests. Machine learning, 45(1), 5-32.

\bibitem[Cohen, 1995]{cohen1995}
Cohen, W. W. (1995). Fast effective rule induction. In Machine learning proceedings 1995 (pp. 115-123). Morgan Kaufmann.

\bibitem[Freund and Schapire, 1997]{freund1997}
Freund, Y., \& Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1), 119-139

\bibitem[Hart, 1968]{hart1968}
Hart, P. (1968). The condensed nearest neighbor rule (Corresp.). IEEE transactions on information theory, 14(3), 515-516.

\bibitem[Minsky, 1961]{minsky1961}
Minsky, M. (1961). Steps toward artificial intelligence. Proceedings of the IRE, 49(1), 8-30


\end{thebibliography}


\end{document}

